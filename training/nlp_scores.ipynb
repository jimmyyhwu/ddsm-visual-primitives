{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score below are fully detailed in the following publication:\n",
    "\n",
    "Shikhar Sharma, Layla El Asri, Hannes Schulz, and Jeremie Zumer. \"Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation\" arXiv preprint arXiv:1706.09799 (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../nlg-eval')\n",
    "from nlgeval import compute_individual_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_reports = joblib.load('../data/predicted_reports_val.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_scores(metrics):\n",
    "    avg = list(metrics.values())[0]\n",
    "    for key, val in list(metrics.items())[1:]:\n",
    "        for idx, item in enumerate(val):\n",
    "            for key2 in item.keys():\n",
    "                avg[idx][key2] += item[key2]\n",
    "    for idx2, score_set in enumerate(avg):\n",
    "        for key3 in score_set.keys():\n",
    "            avg[idx2][key3] = score_set[key3]/len(metrics.keys())\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the w2v embedding of the DeepMiner reports \n",
    "# takes ages to run, this is precomputed cache.\n",
    "# This cache contains the similarity metrics for \n",
    "# each mammogram's ground truth report and predicted report pair.\n",
    "# These metrics include the EmbeddingAverageCosineSimilairty,\n",
    "# GreedyMatchingScore, and VectorExtremaCosineSimilarity.\n",
    "# Skip the next two cells unless you want to get these values again\n",
    "metrics = joblib.load('../data/predicted_reports_w2v_scores_val.jbl' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exhaustive Val Set baseline\n",
    "# Note: check out the requirements in the submodule containing \n",
    "# the Maluuba/nlg-eval repo before running this cell or\n",
    "# the randomized baseline\n",
    "\n",
    "metrics = {}\n",
    "for ind, image_name in enumerate(predicted_reports.keys()):\n",
    "    metrics[image_name] = []\n",
    "    for gt, pred in predicted_reports[image_name]:\n",
    "        try:\n",
    "            metrics_dict = compute_individual_metrics(' '.join(gt), ' '+' '.join(pred), no_overlap=True,\n",
    "                                                      no_skipthoughts=True, no_glove=False)\n",
    "        except:\n",
    "            print('Error! {}, {}'.format(gt, pred))\n",
    "        metrics[image_name].append(metrics_dict)\n",
    "        \n",
    "    if ind%10 == 0:\n",
    "        print('Scored Reports {} / {}'.format(ind, len(predicted_reports.keys())))\n",
    "        print(avg_scores(metrics))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#joblib.dump(metrics, 'predicted_reports_w2v_scores_val.jbl' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized baseline\n",
    "from random import shuffle\n",
    "metrics = {}\n",
    "rand_inds = list(range(len(predicted_reports.keys())))\n",
    "shuffle(rand_inds)\n",
    "reportkeys = list(predicted_reports.keys())\n",
    "for ind, image_name in enumerate(reportkeys):\n",
    "    metrics[image_name] = []\n",
    "    ind2 = 0\n",
    "    for gt, pred_orig in predicted_reports[image_name]:\n",
    "        rind = rand_inds[ind]       \n",
    "        _, pred = predicted_reports[reportkeys[rind]][ind2]\n",
    "        try:\n",
    "            metrics_dict = compute_individual_metrics(' '.join(gt), ' '+' '.join(pred), no_overlap=True,\n",
    "                                                      no_skipthoughts=True, no_glove=False)\n",
    "        except:\n",
    "            print('Error! {}, {}'.format(gt, pred))\n",
    "        metrics[image_name].append(metrics_dict)\n",
    "        ind2 +=1 \n",
    "        \n",
    "    if ind%10 == 0:\n",
    "        print('Scored Reports {} / {}'.format(ind, len(predicted_reports.keys())))\n",
    "        print(avg_scores(metrics))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP scores for predicted reports using top 1 units: {'EmbeddingAverageCosineSimilairty': -0.05958589730961718, 'GreedyMatchingScore': 0.626612518126469, 'VectorExtremaCosineSimilarity': -0.36652118315803106}\n",
      "\n",
      "NLP scores for predicted reports using top 4 units: {'EmbeddingAverageCosineSimilairty': -0.3608542462486459, 'GreedyMatchingScore': 0.5415953116633389, 'VectorExtremaCosineSimilarity': -0.36305756516138443}\n",
      "\n",
      "NLP scores for predicted reports using top 8 units: {'EmbeddingAverageCosineSimilairty': -0.45745307127549334, 'GreedyMatchingScore': 0.5333609578178252, 'VectorExtremaCosineSimilarity': -0.3457480962420635}\n",
      "\n",
      "NLP scores for predicted reports using top 20 units: {'EmbeddingAverageCosineSimilairty': -0.449112173004515, 'GreedyMatchingScore': 0.5604797010213896, 'VectorExtremaCosineSimilarity': -0.3141268791857631}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NLP similarity scores between GT mammogram diagnoses and Predicted DeepMinder reports\n",
    "num_units = [1, 4, 8, 20]\n",
    "for ind, score in enumerate(avg_scores(metrics)):\n",
    "    print('NLP scores for predicted reports using top {} units: {}\\n'.format(num_units[ind], score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
