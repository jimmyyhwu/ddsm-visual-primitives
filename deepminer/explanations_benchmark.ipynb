{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1iqmatynQjfD"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os, sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from munch import Munch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import datasets\n",
    "import fcn_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '../data/ddsm_raw/'\n",
    "image_list_dir = '../data/ddsm_raw_image_lists/'\n",
    "mask_root = '../data/ddsm_masks/3class'\n",
    "config_path = '../training/pretrained/resnet152_3class/config.yml'\n",
    "epoch = 5\n",
    "split = 'test'\n",
    "image_list_path = os.path.join(image_list_dir, '{}.txt'.format(split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path, 'r') as f:\n",
    "    cfg = Munch.fromYAML(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "URE6nP7LQjfG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'resnet152'\n"
     ]
    }
   ],
   "source": [
    "print(\"=> creating model '{}'\".format(cfg.arch.model))\n",
    "if cfg.arch.model == 'resnet152':\n",
    "    model = fcn_resnet.resnet152(num_classes=cfg.arch.num_classes)\n",
    "    features_layer = model.layer4\n",
    "else:\n",
    "    raise Exception\n",
    "\n",
    "model = torch.nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '../training/pretrained/resnet152_3class/checkpoint_00000005.pth.tar'\n",
      "=> loaded checkpoint '../training/pretrained/resnet152_3class/checkpoint_00000005.pth.tar' (epoch 5)\n"
     ]
    }
   ],
   "source": [
    "resume_path = cfg.training.resume.replace(cfg.training.resume[-16:-8], '{:08}'.format(epoch))\n",
    "resume_path = os.path.join('../training', resume_path)\n",
    "if os.path.isfile(resume_path):\n",
    "    print(\"=> loading checkpoint '{}'\".format(resume_path))\n",
    "    checkpoint = torch.load(resume_path)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\".format(resume_path, checkpoint['epoch']))\n",
    "else:\n",
    "    raise Exception(\"=> no checkpoint found at '{}'\".format(resume_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.surgery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "patch_size = 227\n",
    "dataset = datasets.DDSM(data_root, image_list_path, split, patch_size, transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8876a71b959647f699e3f06d516e249b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1044), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipped 276 images in dataset corresponding to normal cases\n"
     ]
    }
   ],
   "source": [
    "# extract features and max activations\n",
    "features = []\n",
    "def feature_hook(module, input, output):\n",
    "    features.extend(output.data.cpu().numpy())\n",
    "features_layer._forward_hooks.clear()\n",
    "features_layer.register_forward_hook(feature_hook)\n",
    "prob_maps = []\n",
    "max_class_probs = []\n",
    "count = 0\n",
    "skipcount = 0\n",
    "extracted_dataset = []\n",
    "with torch.no_grad():\n",
    "    for im_name, image in tqdm(dataset):\n",
    "        # skipping normal cases for text explanation benchmark\n",
    "        if im_name.startswith('normal'):\n",
    "            skipcount += 1\n",
    "            continue\n",
    "        try:\n",
    "            count += 1\n",
    "            input = image.unsqueeze(0)\n",
    "            input = input.cuda()\n",
    "            output = model(input)\n",
    "            output = output.transpose(1, 3).contiguous()\n",
    "            size = output.size()[:3]\n",
    "            output = output.view(-1, output.size(3))\n",
    "            prob = nn.Softmax(dim=1)(output)\n",
    "            prob = prob.view(size[0], size[1], size[2], -1)\n",
    "            prob = prob.transpose(1, 3)\n",
    "            prob = prob.cpu().numpy()\n",
    "            prob_map = prob[0]\n",
    "            prob_maps.append(prob_map)\n",
    "            max_class_probs.append(prob_map.max(axis=(1, 2)))\n",
    "            extracted_dataset.append((im_name, image))\n",
    "        except:\n",
    "            skipcount += 1\n",
    "            continue\n",
    "print(f\"Skipped {skipcount} images in dataset corresponding to normal cases\")\n",
    "max_class_probs = np.array(max_class_probs)\n",
    "image_indices = np.argsort(-max_class_probs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_class_probs = np.array(max_class_probs)\n",
    "image_indices = np.argsort(-max_class_probs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_units = 20\n",
    "params = list(model.parameters())\n",
    "weight_softmax = params[-2].data.cpu().numpy().squeeze(3).squeeze(2)\n",
    "max_activations = np.array([feature_map.max(axis=(1, 2)) for feature_map in features])\n",
    "max_activations = np.expand_dims(max_activations, 1)\n",
    "weighted_max_activations = max_activations * weight_softmax\n",
    "unit_indices = np.argsort(-weighted_max_activations, axis=2)\n",
    "unit_indices = unit_indices[:, :, :num_top_units]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uRIr6sVBQjfE"
   },
   "outputs": [],
   "source": [
    "meta_data = joblib.load('data/ddsm_meta_data.jbl')\n",
    "unit_labels = joblib.load('data/cleaned_unit_labels.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index = 2  # cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3TggE271cYhy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total questions: 416, total answers: 416\n"
     ]
    }
   ],
   "source": [
    "# DeepMiner Benchmark: Text only explanations, \n",
    "# question is \"Does this explanation indicate cancer or benign?\"\n",
    "\n",
    "# Respondants to this benchmark are expected to label cancer/benign \n",
    "# for each case based only on the unit explanations. \n",
    "\n",
    "explanations_benign = []\n",
    "explanations_cancer = []\n",
    "diagnosis = []\n",
    "for count, image_index in enumerate(image_indices[:, class_index]):\n",
    "    image_name, image = extracted_dataset[image_index]\n",
    "    #print(f\"Case #{count}: {image_name}\")\n",
    "    prefix = image_name.split('-')[0][:-3]\n",
    "    try:\n",
    "        gt_report = meta_data['meta'][f'{prefix}s/{image_name}']\n",
    "    except:\n",
    "        #print(f'{image_name} does not have a GT report')\n",
    "        continue\n",
    "    #print('class {} prob: {}'.format(class_index, max_class_probs[image_index][class_index]))\n",
    "    diagnosis.append((count, image_name, gt_report, max_class_probs[image_index][class_index], gt_report[0][-3]))\n",
    "\n",
    "    # Pick top 3 of the top 20 units that have non-empty explanation strings\n",
    "    # Randomly pick 3 (same as # top units) from the list of explained units \n",
    "    # that are not already top units for this mammogram\n",
    "    num_top_units = 3\n",
    "    top_units_w_expl = [] # this are the strings that are the keys for unit_labels (1-based index)\n",
    "\n",
    "    # collect top unit explanations to exclude bottom units with same label\n",
    "    top_explanations = set()\n",
    "    \n",
    "    for unit_index in unit_indices[image_index][class_index]:\n",
    "        try:\n",
    "            #print(f\"Top Unit: {unit_index+1} {unit_labels['unit_'+str(unit_index + 1)]}\")\n",
    "            top_explanations.add(unit_labels['unit_'+str(unit_index + 1)])\n",
    "            top_units_w_expl.append('unit_'+str(unit_index + 1))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    top_units_w_expl = top_units_w_expl[:min(num_top_units,len(top_units_w_expl))]\n",
    "    #print(top_units_w_expl)\n",
    "    deepminer_explanation = []\n",
    "    for unit_name in top_units_w_expl:\n",
    "        unit_index = int(unit_name.split('_')[1])-1\n",
    "        #print(f\"{unit_name} {unit_labels[unit_name]} activation val: {weighted_max_activations[image_index][class_index][unit_index]}\")\n",
    "        deepminer_explanation.append(unit_labels[unit_name])\n",
    "    if prefix == 'cancer':\n",
    "        explanations_cancer.append((count, deepminer_explanation))\n",
    "    else:\n",
    "        explanations_benign.append((count, deepminer_explanation))\n",
    "\n",
    "print(f\"total questions: {len(explanations_cancer) + len(explanations_benign)}, total answers: {len(diagnosis)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate csv for entering text only benchmark reponses\n",
    "import csv\n",
    "\n",
    "# Expects a list of (case_fname, answer_value) tuples\n",
    "def write_csv(csv_fname, header, row_list):\n",
    "    \n",
    "    # Write question csv\n",
    "    with open(csv_fname+'.csv', mode='w') as csv_file:\n",
    "        employee_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        employee_writer.writerow(header)\n",
    "        for row in row_list:\n",
    "            employee_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample same number of cases per class\n",
    "num_cases_per_class = 165\n",
    "random.shuffle(explanations_cancer)\n",
    "random.shuffle(explanations_benign)\n",
    "explanations = explanations_cancer[:num_cases_per_class] + explanations_benign[:num_cases_per_class]\n",
    "random.shuffle(explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "with open('explanations-benchmark.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['Case #', 'DeepMiner Explanation', 'Cancer or Benign (C/B)?']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for e in explanations:\n",
    "        writer.writerow({fieldname: value for fieldname, value in zip(fieldnames, e)})\n",
    "        \n",
    "with open('explanations-benchmark-answer-key.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['Case #', 'Image Name', 'GT Report', 'DeepMiner Cancer Likelihood Prediction', 'GT Diagnosis']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for d in diagnosis:\n",
    "        writer.writerow({fieldname: value for fieldname, value in zip(fieldnames, d)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score responses\n",
    "with open('explanations-benchmark-responses.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    responses = {row['Case #']: row['Cancer or Benign (C/B)?'].upper() for row in reader}\n",
    "with open('explanations-benchmark-answer-key.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    labels = {row['Case #']: 'C' if row['Image Name'].startswith('cancer') else 'B' for row in reader}\n",
    "np.sum([labels[case_num] == response for case_num, response in responses.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Colab_version--deepminer_explanations-randomized_benchmark.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
