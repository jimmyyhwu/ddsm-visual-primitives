{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gen/donuts/mit_backup/gen/ddsm-visual-primitives/deepminer\n"
     ]
    }
   ],
   "source": [
    "cd gen/ddsm-visual-primitives/deepminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models.inception'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a43fd2ff67f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'heatmaps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models.inception'"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "#import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "#from munch import Munch\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "sys.path.append('heatmaps')\n",
    "import models.inception\n",
    "import models.resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = joblib.load('/home/gen/ddsm_meta_data.jbl')\n",
    "unit_labels = joblib.load('/home/gen/cleaned_unit_labels.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.environ.get('DATA_ROOT', '/data/vision/torralba/scratch2/jimmywu/ddsm-visual-primitives/data/raw/')\n",
    "image_list_dir = os.environ.get('IMAGE_LIST_DIR', '/data/vision/torralba/scratch2/jimmywu/ddsm-visual-primitives/data/raw_image_lists/')\n",
    "config_path = os.environ.get('CONFIG_PATH', '/data/vision/torralba/deeprobotics/deeprobotics/ddsm-visual-primitives/ddsm-visual-primitives/training/logs/normal_benign_cancer/2018-02-18_02-08-28.397731_resnet152_pretrained_lr-0.0001_decay-4/config.yml')\n",
    "epoch = int(os.environ.get('EPOCH', '5'))\n",
    "class_index = int(os.environ.get('CLASS_INDEX', '2'))\n",
    "split = os.environ.get('SPLIT', 'val')\n",
    "\n",
    "image_list_path = os.path.join(image_list_dir, '{}.txt'.format(split))\n",
    "image_list_path\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    cfg = Munch.fromYAML(f)\n",
    "\n",
    "if cfg.arch.num_classes == 3:\n",
    "    mask_root = '/data/vision/torralba/scratch2/jimmywu/ddsm-visual-primitives/data/masks/threeclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surgery(model, arch, num_classes):\n",
    "    if arch == 'inception_v3' or arch == 'resnet152':\n",
    "        model.module.fc.cpu()\n",
    "        state_dict = model.state_dict()\n",
    "        state_dict['module.fc.weight'] = state_dict['module.fc.weight'].view(num_classes, 2048, 1, 1)\n",
    "        model.module.fc = nn.Conv2d(2048, num_classes, kernel_size=(1, 1))\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.module.fc.cuda()\n",
    "    else:\n",
    "        raise Exception\n",
    "class DDSM(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, image_list_path, split, patch_size, transform):\n",
    "        self.root = root\n",
    "        with open(image_list_path, 'r') as f:\n",
    "            self.image_names = map(lambda line: line.strip(), f.readlines())\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_names[idx]\n",
    "        image = Image.open(os.path.join(self.root, image_name))\n",
    "        min_dim = min(image.size)\n",
    "        ratio = float(4 * self.patch_size) / min_dim\n",
    "        new_size = (int(ratio * image.size[0]), int(ratio * image.size[1]))\n",
    "        image = image.resize(new_size, resample=Image.BILINEAR)\n",
    "        image = np.asarray(image)\n",
    "        image = np.broadcast_to(np.expand_dims(image, 2), image.shape + (3,))\n",
    "        image = self.transform(image)\n",
    "        return image_name, image        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'resnet152'\n",
      "=> loading checkpoint '/data/vision/torralba/scratch2/jimmywu/ddsm-visual-primitives/checkpoints/normal_benign_cancer/2018-02-18_02-08-28.397731_resnet152_pretrained_lr-0.0001_decay-4/checkpoint_00000005.pth.tar'\n",
      "=> loaded checkpoint '/data/vision/torralba/scratch2/jimmywu/ddsm-visual-primitives/checkpoints/normal_benign_cancer/2018-02-18_02-08-28.397731_resnet152_pretrained_lr-0.0001_decay-4/checkpoint_00000005.pth.tar' (epoch 5)\n"
     ]
    }
   ],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "print(\"=> creating model '{}'\".format(cfg.arch.model))\n",
    "if cfg.arch.model == 'inception_v3':\n",
    "    model = models.inception.inception_v3(use_avgpool=False, transform_input=True)\n",
    "    model.aux_logits = False\n",
    "    model.fc = nn.Linear(2048, cfg.arch.num_classes)\n",
    "    features_layer = model.Mixed_7c\n",
    "elif cfg.arch.model == 'resnet152':\n",
    "    model = models.resnet.resnet152(use_avgpool=False)\n",
    "    model.fc = nn.Linear(2048, cfg.arch.num_classes)\n",
    "    features_layer = model.layer4\n",
    "else:\n",
    "    raise Exception\n",
    "\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "#cudnn.benchmark = True\n",
    "resume_path = cfg.training.resume.replace(cfg.training.resume[-16:-8], '{:08}'.format(epoch))\n",
    "if os.path.isfile(resume_path):\n",
    "    print(\"=> loading checkpoint '{}'\".format(resume_path))\n",
    "    checkpoint = torch.load(resume_path)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\".format(resume_path, checkpoint['epoch']))\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'\".format(resume_path))\n",
    "\n",
    "surgery(model, cfg.arch.model, cfg.arch.num_classes)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "patch_size = 299 if cfg.arch.model == 'inception_v3' else 227\n",
    "dataset = DDSM(data_root, image_list_path, split, patch_size, transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gen/miniconda2/envs/pytorch-0.3/lib/python2.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# extract features and max activations\n",
    "features = []\n",
    "def feature_hook(module, input, output):\n",
    "    features.extend(output.data.cpu().numpy())\n",
    "features_layer._forward_hooks.clear()\n",
    "features_layer.register_forward_hook(feature_hook)\n",
    "prob_maps = []\n",
    "max_class_probs = []\n",
    "for _, image in dataset:\n",
    "    input_var = Variable(image.unsqueeze(0), volatile=True)\n",
    "    output = model(input_var)\n",
    "    output = output.transpose(1, 3).contiguous()\n",
    "    size = output.size()[:3]\n",
    "    output = output.view(-1, output.size(3))\n",
    "    prob = nn.Softmax()(output)\n",
    "    prob = prob.view(size[0], size[1], size[2], -1)\n",
    "    prob = prob.transpose(1, 3)\n",
    "    prob = prob.data.cpu().numpy()\n",
    "    prob_map = prob[0][class_index]\n",
    "    prob_maps.append(prob_map)\n",
    "    max_class_probs.append(prob_map.max())\n",
    "max_class_probs = np.array(max_class_probs)\n",
    "image_indices = np.argsort(-max_class_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = [1, 2, 4, 8, 20]\n",
    "predicted_reports = {}\n",
    "for num_top_units in num_units:\n",
    "    max_activations = np.array([feature_map.max(axis=(1, 2)) for feature_map in features])\n",
    "    #max_activations = np.expand_dims(max_activations, 1)\n",
    "    params = list(model.parameters())\n",
    "    weight_softmax = params[-2].data.cpu().numpy().squeeze(3).squeeze(2)\n",
    "    weighted_max_activations = max_activations * weight_softmax[class_index, :]\n",
    "    #unit_indices = np.argsort(-weighted_max_activations, axis=2)\n",
    "    #unit_indices = unit_indices[:, :, :num_top_units]\n",
    "    unit_indices = np.argsort(-weighted_max_activations, axis=1)\n",
    "    unit_indices = unit_indices[:, :num_top_units]\n",
    "\n",
    "    # reports\n",
    "    for image_index in image_indices: \n",
    "        image_name, image = dataset[image_index]\n",
    "        try:\n",
    "            gt_report = [g[1] for g in meta_data['meta'][image_name]]\n",
    "        except:\n",
    "            # this image didn't have a GT report\n",
    "            continue\n",
    "        indices = unit_indices[image_index]\n",
    "        caption = ' '.join(['unit_{:04}'.format(unit_index + 1) for unit_index in indices])\n",
    "        unit_report = []\n",
    "        for uidx in ['unit_{:04}'.format(unit_index + 1) for unit_index in indices]:\n",
    "            if uidx in unit_labels.keys():\n",
    "                unit_report.append(unit_labels[uidx])\n",
    "        try:\n",
    "            tmp = predicted_reports[image_name]\n",
    "        except:\n",
    "            predicted_reports[image_name] = []\n",
    "        \n",
    "        predicted_reports[image_name].append([gt_report, unit_report])\n",
    "#         print('image {} gt report: {}'.format(image_name, gt_report))\n",
    "#         print('class {} top units: {}'.format(class_index, caption))\n",
    "#         print('unit report: {}'.format(unit_report))    \n",
    "#         print('')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['predicted_reports_val.jbl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(predicted_reports, 'predicted_reports_val.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty Print Reports\n",
    "for image_index in image_indices[0]: # TEST!!\n",
    "    image_name, image = dataset[image_index]\n",
    "    mask_path = os.path.join(mask_root, image_name.replace('jpg', 'png'))\n",
    "    mask = None\n",
    "    if os.path.exists(mask_path):\n",
    "        mask = Image.open(mask_path).resize((image.size(2), image.size(1)))\n",
    "        mask = np.asarray(mask)\n",
    "    for t, m, s in zip(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]):\n",
    "        t.mul_(s).add_(m)\n",
    "    image.clamp_(0, 1)\n",
    "    image = image.numpy().transpose(1, 2, 0)\n",
    "    print('image name: {}'.format(image_name))\n",
    "    print('class {} prob: {}'.format(class_index, max_class_probs[image_index]))\n",
    "    prob_map = prob_maps[image_index]\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(3 * 6, 6))\n",
    "    axes[0].imshow(image)\n",
    "    image_size = image.shape[1::-1]\n",
    "    axes[1].imshow(image)\n",
    "    if mask is not None:\n",
    "        axes[1].imshow(mask == class_index, alpha=0.5, cmap='jet', vmin=0, vmax=1)\n",
    "    heatmap = np.asarray(Image.fromarray(prob_map).resize(image_size, resample=Image.BILINEAR))\n",
    "    axes[2].imshow(image)\n",
    "    axes[2].imshow(heatmap, alpha=0.5, cmap='jet', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    indices = unit_indices[image_index]\n",
    "    caption = ' '.join(['unit_{:04}'.format(unit_index + 1) for unit_index in indices])\n",
    "    print('class {} top units: {}'.format(class_index, caption))\n",
    "    fig, axes = plt.subplots(1, 8, figsize=(8 * 4, 4))\n",
    "    feature_maps = []\n",
    "    top_feature_maps = features[image_index][indices]\n",
    "    top_feature_maps = top_feature_maps - top_feature_maps.min()\n",
    "    top_feature_maps = top_feature_maps / top_feature_maps.max()\n",
    "    for j, unit_index in enumerate(indices):\n",
    "        feature_map = top_feature_maps[j]\n",
    "        image_size = image.shape[1::-1]\n",
    "        feature_map = np.asarray(Image.fromarray(feature_map).resize(image_size, resample=Image.BILINEAR))\n",
    "        feature_maps.append(feature_map)\n",
    "        axes[j].imshow(image)\n",
    "        axes[j].imshow(feature_map, alpha=0.5, cmap='jet', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
